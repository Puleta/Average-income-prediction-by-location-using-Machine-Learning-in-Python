{"cells":[{"cell_type":"markdown","source":["![alt text](http://bigdata.eestec.hu/img/logo_small.png \"MAVE_BIGDATA\")"],"metadata":{}},{"cell_type":"markdown","source":["#  Project Task 1\n## Average income prediction by location using Machine Learning in Python \nThe goal of this competition is that the participants gather environmental and geographical variables that allow the prediction of the average income of residents living in certain areas of Hungary."],"metadata":{}},{"cell_type":"markdown","source":["Avoid using hashtag if you are have a hungarian keyboard. It would remove the current cell.   \n\" [ \" opens a search window. \n\nIf you want make a text box use %md in the first line\n\n..\nhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet"],"metadata":{}},{"cell_type":"markdown","source":["Install [geopandas](http://geopandas.org/) and unidecode  \nUse the \"%%bash\" markup in the first line of the textbox to execute the code as a Unix shell script."],"metadata":{}},{"cell_type":"code","source":["%%bash\n/databricks/python/bin/pip install geopandas\n/databricks/python/bin/pip install unidecode"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Load the python libraries"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nfrom shapely.geometry import Point, Polygon\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn.metrics import mean_absolute_error\nimport unicodedata\nimport unidecode\nimport urllib2\nimport json\nimport time"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### First, we load the data"],"metadata":{}},{"cell_type":"code","source":["%%bash\nfor i in $(ls /dbfs/FileStore/tables/); do\n  echo $i\n  ls /dbfs/FileStore/tables/$i\n  echo ---------------------\ndone"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Load the datasets with income  \nDuring this exercise the competitors have to help solving a data analysis problem from banking. The participants will receive a table partially filled with data about four districts of Budapest (V, VIII, IX, XI) and also four other Hungarian cities (Balatonfüred, Kaposvár, Mosonmagyaróvár, Nyíregyháza).\nThe task is to fill the missing income data with a good prediction and also to add new columns to this table.  \n The area of these places have been divided into cells of 100m×100m size. Every row of the table belongs to such a cell whereas the columns contain values of some variables belonging to these cells.  \n The variables are  \n • CELL ID: unique cell identifier    \n • EOV X: EOV X coordinate of the cell center (units are meters, coordinates in Hungarian standard: https://epsg.io/23700)  \n • EOV Y: EOV Y coordinate of the cell center  \n • Distance from the Danube in meters (we will add during in this example)  \n • Real estate prices in HUF/m2 (we will add during in this example)  \n • Population data (we will add during in this example)  \n • INCOME: average income of bank clients living in the cell (in HUF). 0.0 value in this column codes a missing value."],"metadata":{}},{"cell_type":"code","source":["income = pd.read_csv(\"/dbfs/FileStore/tables/kh54g95h1506348850433/MAVE_POINTS_WITH_INCOME_X_TRAIN_X_TEST2.csv\",encoding='latin-1',sep=',')\nincome.head()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Load the water database"],"metadata":{}},{"cell_type":"code","source":["water_poly = pd.read_csv(\"/dbfs/FileStore/tables/zxn5xi251505301660001/water_2.csv\")\nwater_poly.head()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["# clear accents"],"metadata":{}},{"cell_type":"code","source":["def decode_pandas(row,column):\n  # clear accent\n  if row[column] != 'nan':\n    cleared = unidecode.unidecode(row[column])\n    return cleared\n#  Applies function along input axis of DataFrame.\nincome['NAME'] = income.apply(decode_pandas,column='NAME',axis=1)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["income.NAME.value_counts()\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Translate the column names, and string values to english"],"metadata":{}},{"cell_type":"code","source":["income.NAME = income.NAME.str.replace(\"kerulet\",\"district\")\nincome = income.rename(columns={\"Bp_ker\":\"Bp_district\",\"JOVEDELEM\":\"INCOME\"})\nincome.head()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Lets make a geopandas dataframe"],"metadata":{}},{"cell_type":"code","source":["water_gdf = gpd.GeoDataFrame(water_poly[['AREA','FEAT_TYPE','POLYGON_NM','geometry']])"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# During this workshop we dont have time to understand the followings just accept that \nimport re\n\"\"\"https://pythex.org/\"\"\"\ndef get_poly(row,s):\n    pat = re.compile(r'''(-*\\d+\\.\\d+ -*\\d+\\.\\d+),*''')\n    split_pat = '[)].+?[(]'\n    matched_list = re.split(split_pat,row[s])\n    lst = list()\n    if len(matched_list) > 1:\n        step = 0\n        list_other = []\n        for line in matched_list:\n            line_matched = re.findall(pat,line)\n            if step == 0:\n                first = [tuple([float(m.split()[0]),float(m.split()[1])]) for m in line_matched]\n                step = 1\n            else:\n                list_other.append([tuple([float(m.split()[0]),float(m.split()[1])]) for m in line_matched])\n\n        lst = Polygon(first,list_other)\n            \n    else:\n        line_matched = re.findall(pat,matched_list[0])\n        lst = Polygon([tuple([float(m.split()[0]),float(m.split()[1])]) for m in line_matched])\n        #http://toblerity.org/shapely/manual.html#polygons\n    return lst\n  \ndef getPolyCoords(row, geom, coord_type):\n    \"\"\"Returns the coordinates ('x' or 'y') of edges of a Polygon exterior\"\"\"\n\n    # Parse the exterior of the coordinate\n    exterior = row[geom].exterior\n\n    if coord_type == 'x':\n        # Get the x coordinates of the exterior\n        return list( exterior.coords.xy[0] )\n    elif coord_type == 'y':\n        # Get the y coordinates of the exterior\n        return list( exterior.coords.xy[1] )"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#Lets make a geodataframe with geometry\nwater_gdf['geometry'] =  water_gdf.apply(get_poly, s='geometry', axis = 1)\n# Get the Polygon x and y coordinates\nwater_gdf['x'] = water_gdf.apply(getPolyCoords, geom='geometry', coord_type='x', axis=1)\nwater_gdf['y'] = water_gdf.apply(getPolyCoords, geom='geometry', coord_type='y', axis=1)\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Display the water polygon"],"metadata":{}},{"cell_type":"code","source":["x = water_gdf['x'].values\ny = water_gdf['y'].values\n\n\nfig, ax = plt.subplots()\nfor num in range(len(x)):\n    ax.plot(x[num], y[num], 'k-')\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["\"\"\"income['geometry'] =  [Point(xy) for xy in zip(income.x, income.y)]\nincome = gpd.GeoDataFrame(income)\nto_crs = {'init':'epsg:23700'}\ncrs = {'init':'epsg:4326'}\nwater_gdf.crs = crs\nwater_gdf = water_gdf.to_crs(to_crs)\nincome.crs=to_crs\nincome = income.to_crs(crs)\"\"\""],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["water_gdf.geometry"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Change Coordinate Reference Systems (crs) to hungarian epsg:23700\nhttps://epsg.io/23700"],"metadata":{}},{"cell_type":"code","source":["to_crs = {'init':'epsg:23700'}\ncrs = {'init':'epsg:4326'}\nwater_gdf.crs = crs\nwater_gdf = water_gdf.to_crs(to_crs)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["water_gdf.geometry"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["def getPointCoords(row, geom, coord_type):\n    #Calculates coordinates ('x' or 'y') of a Point geometry\n    if coord_type == 'x':\n        return row[geom].x\n    elif coord_type == 'y':\n        return row[geom].y\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["water_gdf.head()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# drop NA/Nan values\nwater_gdf = water_gdf.dropna(axis = 0, subset=[\"POLYGON_NM\"])\nwater_gdf = water_gdf.dropna(axis = 0, subset=[\"geometry\"])\n#Make eov x and y\nwater_gdf['x'] = water_gdf.apply(getPolyCoords, geom='geometry', coord_type='x', axis=1)\nwater_gdf['y'] = water_gdf.apply(getPolyCoords, geom='geometry', coord_type='y', axis=1)\n#Select Danube \ndanube = water_gdf[water_gdf.POLYGON_NM.str.contains(\"DUNA\")]\ndanube.head()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Add distance between Danube and points"],"metadata":{}},{"cell_type":"code","source":["def get_dist(row,x,y):\n  # The distance is calculated from the nearest boundary [m]\n  dist = (np.min(danube.boundary.distance(Point(row[x],row[y])).values))\n  return dist\n\nincome['distance_from_Danube'] = income.apply(get_dist,x='x',y='y',axis=1)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["income[['distance_from_Danube','geometry']].sort_values('distance_from_Danube',ascending=True)\n"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["x = danube['x'].values\ny = danube['y'].values\nBp = income[income.Bp_district.notnull()]\n\nfig, ax = plt.subplots()\nfor num in range(len(x)):\n    ax.plot(x[num], y[num], 'k-')\nax.plot(Bp.x, Bp.y, 'ro')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Open real estate and population csv"],"metadata":{}},{"cell_type":"code","source":["realestate_pop = pd.read_csv(\"/dbfs/FileStore/tables/vg2clejn1505380421340/ingatlan_and_dem_for_mave.csv\",encoding=\"latin-1\")\nrealestate_pop.head()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Translate to english"],"metadata":{}},{"cell_type":"code","source":["hun_dict = {\"_0_14eves\":\"_0_14_years_old\",\"_15_18eves\":\"_15_18_years_old\",\"_19_30eves\":\"_19_30_years_old\",\"_31_62eves\":\"_31_62_years_old\",\"_63_Xeves\":\"_63_Xyears_old\",\\\n           \"kerulet\":\"district\",\"nepesseg\":\"population\",\"nmar\":\"square_meter_price\",\"telepules\":\"settlement\"}\nrealestate_pop = realestate_pop.rename(columns=hun_dict)\nrealestate_pop.head(1)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["Join real estate square meter price to the income points"],"metadata":{}},{"cell_type":"code","source":["realestate_pop = realestate_pop.reset_index(drop=True)\nincome = income.reset_index(drop=True)\nX=realestate_pop.loc[:,['x','y']]\n#http://scikit-learn.org/stable/modules/neighbors.html\nnbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree',n_jobs=-1).fit(X)\ndistances, indices = nbrs.kneighbors(income.loc[:,['x','y']])\nprint(len(indices))\nprint(len(distances))\n"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["indices.flatten()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["#Make a dataframe with square_meter_price\nsq_meter_price = pd.DataFrame(realestate_pop.square_meter_price[indices.flatten()])\nsq_meter_price = sq_meter_price.reset_index(drop=True)\nincome['square_meter_price'] = sq_meter_price['square_meter_price']\n#Make a dataframe with population\npopulation = pd.DataFrame(realestate_pop.population[indices.flatten()])\npopulation = population.reset_index(drop=True)\nincome['population'] = population['population']\nincome['DISTANCE_FROM_REALESTATE_POINTS'] = distances\nincome.head()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["# Use overpy (overpass) api to download POI-s  ![alt text](http://wiki.openstreetmap.org/w/images/thumb/b/b5/Overpass_API_logo.svg/300px-Overpass_API_logo.svg.png)\nhttp://overpass-turbo.eu/  \nhttp://wiki.openstreetmap.org/wiki/Overpass_API"],"metadata":{}},{"cell_type":"code","source":["%%bash\n/databricks/python/bin/pip install overpy"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["import overpy\n\napi = overpy.Overpass()\n\n#you can test in the link above \nquery = '''area[name=\"Magyarország\"]->.a;\n           node(area.a)[amenity=cafe];out meta;'''\nresult = api.query(query)\nprint(len(result.nodes))\nprint(len(result.ways))\nprint(len(result.relations))\n\nlista = []\nfor node in result.nodes:\n    print('------------------------------------')\n    print(node.tags)\n    tgs = node.tags\n    tgs['lat'] = node.lat\n    tgs['lon'] = node.lon\n\n    print((node.lat, node.lon))\n    lista.append(tgs)\n    \ncafe_df = pd.DataFrame(lista)\n\nprint(len(cafe_df))\ncafe_df.head()\n"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["cafe_df.to_pickle(\"/dbfs/FileStore/tables/cafe.p\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["cf_df = cafe_df.loc[:,['name','lat','lon','amenity','wifi','opening_hours']].copy()\ncf_df.head()\n"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["# drop NA/Nan values\ncf_df = cf_df.dropna(axis = 0, subset=[\"lat\"])\ncf_df = cf_df.dropna(axis = 0, subset=[\"lon\"])\ncf_df = cf_df.dropna(axis = 0, subset=[\"name\"])"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["cf_df.name = cf_df.apply(decode_pandas,column='name',axis=1)\ncf_df = cf_df.fillna(value=\"no\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["cf_df.wifi.value_counts()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["cf_df.wifi = cf_df.wifi.str.replace(\"free\",\"yes\")"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["## Dont forget to change crs to Hungarian format!  \nFirst we have to make a geopandas format file"],"metadata":{}},{"cell_type":"code","source":["#add_geometry_to_dataframe and return geopandas objectum\ndef add_geometry_to_dataframe(df,Lon,Lat,crs):\n    geometry = [Point(xy) for xy in zip(df[Lat], df[Lon])]\n    geo_df = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n    return geo_df"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["crs_overpy = {'init':'epsg:4326'} #like a GPS format\ncafe_gdf = add_geometry_to_dataframe(cf_df,'lat','lon',crs_overpy)\nprint(\"the cafe_gdf crs is {}\".format(cafe_gdf.crs))\ncafe_gdf.head()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["Now we can change the crs to EOV format"],"metadata":{}},{"cell_type":"code","source":["to_crs = {'init':'epsg:23700'}\ncafe_gdf = cafe_gdf.to_crs(to_crs)\ncafe_gdf.head()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["Add new columns x,y"],"metadata":{}},{"cell_type":"code","source":["cafe_gdf['x'] = cafe_gdf.apply(getPointCoords, geom='geometry', coord_type='x', axis=1)\ncafe_gdf['y'] = cafe_gdf.apply(getPointCoords, geom='geometry', coord_type='y', axis=1)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["cafe_gdf.head()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["link the coffee to income cell using the NearestNeighbors method"],"metadata":{}},{"cell_type":"code","source":["cafe_gdf = cafe_gdf.reset_index(drop=True)\n\nX=cafe_gdf.loc[:,['x','y']]\n#http://scikit-learn.org/stable/modules/neighbors.html\nnbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree',n_jobs=-1).fit(X)\ndistances, indices = nbrs.kneighbors(income.loc[:,['x','y']])\n\ncafe_name = pd.DataFrame(cafe_gdf.name[indices.flatten()])\ncafe_name = cafe_name.reset_index(drop=True)\nincome['cafe_name'] = cafe_name['name']\n\nwifi = pd.DataFrame(cafe_gdf.wifi[indices.flatten()])\nwifi = wifi.reset_index(drop=True)\nincome['cafe_with_wifi'] = wifi['wifi']\n\nincome['DISTANCE_FROM_NEAREST_CAFE'] = distances\nincome.head()"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["# Use Google Api![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQuGeMVtXFzYYdONQAw3hGmlSD0ciozncVYoQw2lSUIHLDNjetl)\n\nDownload coordinates of GYMs\nIn this tutorial we'll be using Google's API to retreive points-of-interests (POIs) in the vicinity of a geographical coordinate.\n\nPlease read the short introduction here: https://developers.google.com/places/web-service/intro\n\nIn order that you can use the API, you need to create an API key.   \nTo do so, click \"Get a Key\" under \"Get an API Key\" and follow the steps.  \nhttps://developers.google.com/places/web-service/get-api-key Trynot to forget this key, as you'll need it.  \nIt will look like something like this: AIzbSyDZ8GUG1jOR-6CkI2UxhsL5SV-3LILfjSU (I deliberately changed a few characters, so this is NOT a valid key.)\n\nBefore going further, please read the tutorial here: https://developers.google.com/places/web-service/search"],"metadata":{}},{"cell_type":"markdown","source":["First we have to create GPS coordinates"],"metadata":{}},{"cell_type":"code","source":["#\nincome_gps = income.copy()\ncrs_EOV = {'init':'epsg:23700'} #\ncrs_GPS = {'init':'epsg:4326'} #like a GPS format\nincome_gps = add_geometry_to_dataframe(income_gps,'y','x',crs_EOV)\nprint(\"the income_gps crs is {}\".format(income_gps.crs))\nincome_gps = income_gps.to_crs(crs_GPS)\nprint(\"the income_gps crs is {}\".format(income_gps.crs))\n# Make lat, lon columns\nincome_gps['lat'] = income_gps.apply(getPointCoords, geom='geometry', coord_type='y', axis=1)\nincome_gps['lon'] = income_gps.apply(getPointCoords, geom='geometry', coord_type='x', axis=1)\n\n\nincome_gps.head()\n\n"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["one example how the google api works"],"metadata":{}},{"cell_type":"code","source":["location = \"47.4956819,19.0519238\"\nradius = \"50000\" \ntypes = \"gym\" #https://developers.google.com/places/web-service/supported_types\nname = \"\"\nkey = 'AIzaSyCcUb6HzIrnkFO5DcMSq18LfjxKtHJ6gw8'\nurl = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=\" + \\\nlocation + \\\n\"&radius=\" + radius + \\\n\"&types=\" + types + \\\n\"&name=\" + name + \\\n\"&key=\" + key\nprint(url)\nheaders = {}\nheaders['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\nreq = urllib2.Request(url, headers = headers)\nresp = urllib2.urlopen(req)\nfile0 = json.loads(resp.read().decode('utf-8'))\ndf_lista = []\nfor line in file0['results']:\n  print(line['name'])\n  print(\"----------------------\")\n  df_lista.append([line['name'],line['geometry']['location']['lat'],line['geometry']['location']['lng'],line['place_id']] )\napi_df = pd.DataFrame(df_lista,columns=['name','lat','long','place_id'])  \napi_df.head()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["Note: Nearby search and Text search return at most 20 results per query, giving you a next_page_token, if there are more results. (However maximum 60 results are returned for a given search.) See this post about the usage of the next_page_token: https://stackoverflow.com/questions/9614258/how-to-get-20-result-from-google-places-api"],"metadata":{}},{"cell_type":"code","source":["def use_google_api(lat,lon,radius,types,name,key,sleep=4):\n  try:\n      location = str(lat) + \",\" + str(lon)\n      url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=\" + \\\n      location + \\\n      \"&radius=\" + radius + \\\n      \"&types=\" + types + \\\n      \"&name=\" + name + \\\n      \"&key=\" + key\n      headers = {}\n      '''Before making the URL request, we'll attach a header that tricks Google into thinking we're a browser,\n      not an amateur programmer making dumb requests, waiting to be blocked by Google's servers. \n      However it has to be said, that  Google is clever enough to notice that if you're making too many requests,\n      you surely can't be a man sitting in fron of a web browser.'''\n      headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\n      req = urllib2.Request(url, headers = headers) #Combine the header and the URL into a request\n      resp = urllib2.urlopen(req) #Make the URL request\n      file0 = json.loads(resp.read().decode('utf-8')) #Decode the answer given \n      df_lista = []\n      for line in file0['results']:\n        df_lista.append([line['name'],line['geometry']['location']['lat'],line['geometry']['location']['lng'],line['place_id']] )\n      api_df = pd.DataFrame(df_lista,columns=['name','lat','long','place_id'])  \n      return api_df\n      time.sleep(sleep)\n  except Exception as e:\n      print(e)\n      return False\n\n  "],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["income_gps.iloc[10].lat"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["radius = \"50000\"\ntypes = \"gym\" #https://developers.google.com/places/web-service/supported_types\nname = \"\"\nkey = 'AIzaSyCcUb6HzIrnkFO5DcMSq18LfjxKtHJ6gw8'\ngoogle_df = pd.DataFrame()\nfor x in range(0,len(income_gps),100):\n    \n    google_df_2 = use_google_api(income_gps.iloc[x].lat,income_gps.iloc[x].lon,radius,types,name,key,sleep=4)\n    if len(google_df_2) > 0:\n      google_df = google_df.append(google_df_2)\n      \ngoogle_df.head()\n  "],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["# Drop duplicates\nprint(len(google_df))\ngoogle_df = google_df.drop_duplicates(subset=['name','lat','long'])\nprint(len(google_df))\ncrs_EOV = {'init':'epsg:23700'} #\ncrs_GPS = {'init':'epsg:4326'} #like a GPS format\ngoogle_df = add_geometry_to_dataframe(google_df,'lat','long',crs_GPS)\nprint(\"the google_df crs is {}\".format(google_df.crs))\n#Change crs to EOV format\ngoogle_df = google_df.to_crs(crs_EOV)\nprint(\"the google_df crs is {}\".format(google_df.crs))\n# Make x, y columns\ngoogle_df['x'] = google_df.apply(getPointCoords, geom='geometry', coord_type='x', axis=1)\ngoogle_df['y'] = google_df.apply(getPointCoords, geom='geometry', coord_type='y', axis=1)\n\ngoogle_df.head()\n"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["google_df = google_df.reset_index(drop=True)\n\nX=google_df.loc[:,['x','y']]\n#http://scikit-learn.org/stable/modules/neighbors.html\nnbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree',n_jobs=-1).fit(X)\ndistances, indices = nbrs.kneighbors(income.loc[:,['x','y']])\n\n\nincome['DISTANCE_FROM_NEAREST_GYM'] = distances\nincome.head()\n"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["# Do some machine learning\n![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSJa4wDSG7s6rtfe4P4uf-hVAAF_SVFtCvMsUZ-DnR2TjKsJ0g4DA)"],"metadata":{}},{"cell_type":"code","source":["# copy the important features\ndf = income[['NAME','ZIP','INCOME','square_meter_price','population','distance_from_Danube','DISTANCE_FROM_NEAREST_CAFE','cafe_with_wifi','cafe_name','DISTANCE_FROM_NEAREST_GYM']]"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["# Replace categorical data with one-hot encoded data\nfeatures_df = pd.get_dummies(df, columns=['NAME', 'ZIP','cafe_with_wifi','cafe_name'])\n# Remove the sale price from the feature data\ndel features_df['INCOME']\nfeatures_df.head(2)"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["features_df[features_df['NAME_KAPOSVAR'] > 0].head()"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["# Create the X and y arrays\nX = features_df.as_matrix()\ny = df['INCOME'].as_matrix()"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["# Split the data set in a training set (70%) and a test set (30%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2017)\nprint(\"X_train len is {}\".format(len(X_train)))\nprint(\"X_test len is {}\".format(len(X_test)))"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["Check the y_test   \nit's sum has to be 0"],"metadata":{}},{"cell_type":"code","source":["print(\"y_test sum is {}\".format((np.sum(y_test))))\nprint(\"y_train sum is {}\".format((np.sum(y_train))))"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["income['JOVEDELEM'][income.index_2.isin(X_test[:,0])] = 0"],"metadata":{}},{"cell_type":"code","source":["# Fit regression model -- Andras Horvath will speak about it \nmodel = ensemble.GradientBoostingRegressor(\n    n_estimators=1000,\n    learning_rate=0.1,\n    max_depth=6,\n    min_samples_leaf=9,\n    max_features=0.8,\n    loss= 'lad',\n    random_state=700\n)\n\"\"\"criterion = \"mae\",\"\"\"\nmodel.fit(X_train, y_train)"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["# Find the error rate on the training set\nmse = mean_absolute_error(y_train, model.predict(X_train))\nprint(\"Training Set Mean Absolute Error: %.4f\" % mse)\n\n\"\"\"# Find the error rate on the test set\nmse = mean_absolute_error(y_test, model.predict(X_test))\nprint(\"Test Set Mean Absolute Error: %.4f\" % mse)\"\"\""],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["# Lets predict the y_test"],"metadata":{}},{"cell_type":"code","source":["print(len(model.predict(X_test)))\ny_test_predicted = pd.DataFrame(model.predict(X_test),columns=[\"y_test_predicted\"])\ny_test_predicted.head()"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["y = model.predict(X_train)\nx = y_train\n\n\nfig, ax = plt.subplots()\n\"\"\"ax.plot(x[num], y[num], 'k-')\"\"\"\nax.scatter(x,y)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["# These are the feature labels from our data set\nfeature_labels = features_df.columns.values\n\n# Create a numpy array based on the model's feature importances\nimportance = model.feature_importances_\n\n# Sort the feature labels based on the feature importance rankings from the model\nfeauture_indexes_by_importance = importance.argsort()"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["# Print each feature label, from most important to least important (reverse order)\nfor index in feauture_indexes_by_importance:\n    print(\"{} - {:.2f}%\".format(feature_labels[index], (importance[index] * 100.0)))"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":["### Save the y_test file to csv"],"metadata":{}},{"cell_type":"code","source":["%%bash\nmkdir /dbfs/FileStore/my-stuff"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["#https://docs.databricks.com/user-guide/advanced/filestore.html\n\"\"\"If you are on Community Edition you need to replace https://community.cloud.databricks.com/files/my-stuff/my-file.txt with https://community.cloud.databricks.com/files/my-stuff/my-file.txt?o=###### where the number after o= is the same as in your Community Edition URL.\"\"\"\n\ny_test_predicted.to_csv(\"/dbfs/FileStore/my-stuff/y_test_predicted.txt\")\n"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["(https://community.cloud.databricks.com/files/my-stuff/y_test_predicted.txt?o=3778117289135480)"],"metadata":{}},{"cell_type":"markdown","source":["Download as txt file  \nright click and save as  \n![alt text](http://www.homeandlearn.co.uk/JS/images/templates/chrome_save_as.gif)\n![alt text](http://www.energialternativa.org/yabbfiles/Attachments/GIF--Clap-applause-good-job-nice-one-clapping-Despicable-Me-Minion-Minions-GIF_001.gif)"],"metadata":{}},{"cell_type":"markdown","source":["# Project exercise #2\nAs a competition running parallel to the previous one, we would like to offer a prize also for ideas  \nthat could not have been realized during the workshop for reasons such as lack of time or technical difficulties.  \nIn this case a competing team has to hand in:   \n• The name of the variable.  \n• A source where it can be legally accessed. (Including an URL.)  \n• A sample of the data.  \n• The method of creating a variable from the data.  \n• Reasons why this variable could be important.   \n  \nSolutions to this task in ppt or pdf format are to be submitted by 5th October, 2017. 15.00 to the following e-mail address: Aron.Asztalos@otpbank.hu"],"metadata":{}},{"cell_type":"markdown","source":["For example:  \n__[airbnb](https://www.airbnb.com/s/Baltonf%C3%BCred/homes?checkin=2017-10-12&checkout=2017-10-21&allow_override%5B%5D=&s_tag=OefP-q4D&ib=true)__  \n\n  https://www.airbnb.hu/api/v2/explore_tabs?version=1.2.5&_format=for_explore_search_web&items_per_grid=18&experiences_per_grid=20&guidebooks_per_grid=20&fetch_filters=true&supports_for_you_v3=true&screen_size=large&timezone_offset=120&auto_ib=true&guest_from_sem_traffic=false&is_luxury_request=false&tab_id=home_tab&location=Budapest&allow_override%5B%5D=&ne_lat=47.57694591928604&ne_lng=19.138997001941107&sw_lat=47.52581999795169&sw_lng=19.057114525134466&zoom=13&search_by_map=true&federated_search_session_id=757552d5-d87d-41e5-9ad7-0b67f5d5b11d&_intents=p1&key=d306zoyjsyarp7ifhu67rjxn52tv0t20&currency=HUF&locale=hu\n  \n__[satellite images](https://scihub.copernicus.eu/dhus/#/home)__\n\n__[best practise](http://bfy.tw/ECaX)__"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":98}],"metadata":{"name":"MAVE_PROJECT_JOB","notebookId":2098107784732666},"nbformat":4,"nbformat_minor":0}
